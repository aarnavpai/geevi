<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine-learning on Girish Varma</title>
    <link>https://girishvarma.in/projects/machine-learning/</link>
    <description>Recent content in machine-learning on Girish Varma</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Jan 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://girishvarma.in/projects/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://girishvarma.in/students/aditya-kallapa/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/students/aditya-kallapa/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://girishvarma.in/students/furqan-sheik/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/students/furqan-sheik/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://girishvarma.in/students/garima-nishad/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/students/garima-nishad/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://girishvarma.in/students/prateek-pani/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/students/prateek-pani/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://girishvarma.in/students/sandeep-kumar/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/students/sandeep-kumar/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic Graphical Models</title>
      <link>https://girishvarma.in/teaching/prob-graph-models/</link>
      <pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/teaching/prob-graph-models/</guid>
      <description>Probabilistic Graphical Models refers to  i.) concise representations of probability distributions using graphs ii.) efficient algorithms for sampling distributions represented in such form iii.) learning these representations from data.</description>
    </item>
    
    <item>
      <title>Semantic Segmentation Datasets for Resource Constrained Training</title>
      <link>https://girishvarma.in/publication/idd-lite/</link>
      <pubDate>Tue, 22 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/idd-lite/</guid>
      <description>Several large scale datasets, coupled with advances in deep neural network architectures have been greatly successful in pushing the boundaries of performance in semantic segmentation in recent years. However, the scale and magnitude of such datasets prohibits ubiquitous use and widespread adoption of such models, especially in settings with serious hardware and software resource constraints. Through this work, we propose two simple variants of the recently proposed IDD dataset, namely IDD-mini and IDD-lite, for scene understanding in unstructured environments.</description>
    </item>
    
    <item>
      <title>Dynamic Block Sparse Reparametarization of Convolutional Neural Networks</title>
      <link>https://girishvarma.in/publication/block-sparse/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/block-sparse/</guid>
      <description>Sparse neural networks are efficient in both memory and compute when compared to dense neural networks. But on parallel hardware such as GPU, sparse neural networks result in small or no runtime performance gains. On the other hand, structured sparsity patterns like filter, channel and block sparsity result in large performance gains due to regularity induced by structure. Among structured sparsities, block sparsity is a generic structured sparsity pattern with filter and channel sparsity being sub cases of block sparsity.</description>
    </item>
    
    <item>
      <title>Universal Semi-supervised Semantic Segmentation</title>
      <link>https://girishvarma.in/publication/univ-seg/</link>
      <pubDate>Fri, 16 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/univ-seg/</guid>
      <description>In recent years, the need for semantic segmentation has arisen across several different applications and environments. However, the expense and redundancy of annotation often limits the quantity of labels available for training in any domain, while deployment is easier if a single model works well across domains. In this paper, we pose the novel problem of universal semi-supervised semantic segmentation and propose a solution framework, to meet the dual needs of lower annotation and deployment costs.</description>
    </item>
    
    <item>
      <title>Class2Str: End to End Latent Hierarchy Learning</title>
      <link>https://girishvarma.in/publication/class2str/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/class2str/</guid>
      <description>Deep neural networks for image classification typically consists of a convolutional feature extractor followed by a fully connected classifier network. The predicted and the ground truth labels are represented as one hot vectors. Such a representation assumes that all classes are equally dissimilar. However, classes have visual similarities and often form a hierarchy. Learning this latent hierarchy explicitly in the architecture could provide invaluable insights. We propose an alternate architecture to the classifier network called the Latent Hierarchy (LH) Classifier and an end to end learned Class2Str mapping which discovers a latent hierarchy of the classes.</description>
    </item>
    
  </channel>
</rss>