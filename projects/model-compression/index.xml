<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>model-compression on Girish Varma</title>
    <link>https://girishvarma.in/projects/model-compression/</link>
    <description>Recent content in model-compression on Girish Varma</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://girishvarma.in/projects/model-compression/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Ramanujan Bipartite Graph Products for Efficient Block Sparse Neural Networks</title>
      <link>https://girishvarma.in/publication/rbgp/</link>
      <pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/rbgp/</guid>
      <description>Sparse neural networks are shown to give accurate predictions competitive to denser versions, while also minimizing the number of arithmetic operations performed. However current hardware like GPU&amp;rsquo;s can only exploit structured sparsity patterns for better efficiency. Hence the run time of a sparse neural network may not correspond to the arithmetic operations required.
In this work, we propose RBGP( Ramanujan Bipartite Graph Product) framework for generating structured multi level block sparse neural networks by using the theory of Graph products.</description>
    </item>
    
    <item>
      <title>Semantic Segmentation Datasets for Resource Constrained Training</title>
      <link>https://girishvarma.in/publication/idd-lite/</link>
      <pubDate>Tue, 22 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/idd-lite/</guid>
      <description>Several large scale datasets, coupled with advances in deep neural network architectures have been greatly successful in pushing the boundaries of performance in semantic segmentation in recent years. However, the scale and magnitude of such datasets prohibits ubiquitous use and widespread adoption of such models, especially in settings with serious hardware and software resource constraints. Through this work, we propose two simple variants of the recently proposed IDD dataset, namely IDD-mini and IDD-lite, for scene understanding in unstructured environments.</description>
    </item>
    
    <item>
      <title>Dynamic Block Sparse Reparametarization of Convolutional Neural Networks</title>
      <link>https://girishvarma.in/publication/block-sparse/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/block-sparse/</guid>
      <description>Sparse neural networks are efficient in both memory and compute when compared to dense neural networks. But on parallel hardware such as GPU, sparse neural networks result in small or no runtime performance gains. On the other hand, structured sparsity patterns like filter, channel and block sparsity result in large performance gains due to regularity induced by structure. Among structured sparsities, block sparsity is a generic structured sparsity pattern with filter and channel sparsity being sub cases of block sparsity.</description>
    </item>
    
    <item>
      <title>Deep Expander Networks: Efficient Deep Networks from Graph Theory</title>
      <link>https://girishvarma.in/publication/xnet/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/xnet/</guid>
      <description>Efficient CNN designs like ResNets and DenseNet were proposed to improve accuracy vs efficiency trade-offs. They essentially increased the connectivity, allowing efficient information flow across layers. Inspired by these techniques, we propose to model connections between filters of a CNN using graphs which are simultaneously sparse and well connected. Sparsity results in efficiency while well connectedness can preserve the expressive power of the CNNs. We use a well-studied class of graphs from theoretical computer science that satisfies these properties known as Expander graphs.</description>
    </item>
    
    <item>
      <title>Efficient Semantic Segmentation using Gradual Grouping</title>
      <link>https://girishvarma.in/publication/grad-group/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/grad-group/</guid>
      <description>Deep CNNs for semantic segmentation have high memory and run time requirements. Various approaches have been proposed to make CNNs efficient like grouped, shuffled, depth-wise separable convolutions. We study the effectiveness of these techniques on a real-time semantic segmentation architecture like ERFNet for improving runtime by over 5X. We apply these techniques to CNN layers partially or fully and evaluate the testing accuracies on Cityscapes dataset. We obtain accuracy vs parameters/FLOPs trade offs, giving accuracy scores for models that can run under specified runtime budgets.</description>
    </item>
    
    <item>
      <title>Class2Str: End to End Latent Hierarchy Learning</title>
      <link>https://girishvarma.in/publication/class2str/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/class2str/</guid>
      <description>Deep neural networks for image classification typically consists of a convolutional feature extractor followed by a fully connected classifier network. The predicted and the ground truth labels are represented as one hot vectors. Such a representation assumes that all classes are equally dissimilar. However, classes have visual similarities and often form a hierarchy. Learning this latent hierarchy explicitly in the architecture could provide invaluable insights. We propose an alternate architecture to the classifier network called the Latent Hierarchy (LH) Classifier and an end to end learned Class2Str mapping which discovers a latent hierarchy of the classes.</description>
    </item>
    
    <item>
      <title>Efficient CNNs</title>
      <link>https://girishvarma.in/teaching/efficient-cnns/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/teaching/efficient-cnns/</guid>
      <description>Surveying methods used to make deep learning models efficient.</description>
    </item>
    
    <item>
      <title>Compressing Models for Recognizing Places</title>
      <link>https://girishvarma.in/publication/compression-place/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/compression-place/</guid>
      <description>Visual place recognition on low memory devices such as mobile phones and robotics systems is a challenging problem. The state of the art models for this task uses deep learning architectures having close to 100 million parameters which takes over 400MB of memory. This makes these models infeasible to be deployed in low memory devices and gives rise to the need of compressing them. Hence we study the effectiveness of model compression techniques like trained quantization and pruning for reducing the number of parameters on one of the best performing image retrieval models called NetVLAD.</description>
    </item>
    
  </channel>
</rss>