<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gv on gv</title>
    <link>http://geevi.github.io/</link>
    <description>Recent content in gv on gv</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Girish Varma</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0530</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Deep Expander Networks: Efficient Deep Networks from Graph Theory</title>
      <link>http://geevi.github.io/publication/xnet/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/xnet/</guid>
      <description>&lt;p&gt;Deep Neural Networks, while being unreasonably effective for several vision tasks, have their usage limited by the computational and memory requirements, both during training and inference stages. Analyzing and improving the connectivity patterns between layers of a network has resulted in several compact architectures like GoogleNet, ResNet and DenseNet-BC. In this work, we utilize results from graph theory to develop an efficient connection pattern between consecutive layers. Specifically, we use {\it expander graphs} that have excellent connectivity properties to develop a sparse network architecture, the deep expander network (X-Net). The X-Nets are shown to have high connectivity for a given level of sparsity. We also develop highly efficient training and inference algorithms for such networks. Experimental results show that we can achieve the similar or better accuracy as DenseNet-BC with two-thirds the number of parameters and FLOPs on several image classification benchmarks. We hope that this work motivates other approaches to utilize results from graph theory to develop efficient network architectures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>XNet : Efficient Deep Networks</title>
      <link>http://geevi.github.io/project/xnet/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/xnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Compressing Models for Recognizing Places</title>
      <link>http://geevi.github.io/publication/compression-place/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/compression-place/</guid>
      <description>&lt;p&gt;Visual place recognition on low memory devices such as mobile phones and robotics systems is a challenging problem. The state of the art models for this task uses deep learning architectures having close to 100 million parameters which takes over 400MB of memory. This makes these models infeasible to be deployed in low memory devices and gives rise to the need of compressing them. Hence we study the effectiveness of model compression techniques like trained quantization and pruning for reducing the number of parameters on one of the best performing image retrieval models called NetVLAD. We show that a compressed network can be created by starting with a model pre-trained for the task of visual place recognition and then fine-tuning it via trained pruning and quantization. The compressed model is able to produce the same mAP as the original uncompressed network. We achieve almost 50% parameter pruning with no loss in mAP and 70% pruning with close to 2% mAP reduction, while also performing 8-bit quantization. Furthermore, together with 5-bit quantization, we perform about 50% parameter reduction by pruning and get only about 3% reduction in mAP.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Super-polylogarithmic hypergraph coloring hardness via low-degree long codes</title>
      <link>http://geevi.github.io/publication/hyper-coloring-journal/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/hyper-coloring-journal/</guid>
      <description>&lt;p&gt;We prove improved inapproximability results for hypergraph coloring using the low-degree polynomial code (aka, the &amp;lsquo;short code&amp;rsquo; of Barak et. al. [FOCS 2012]) and the techniques proposed by Dinur and Guruswami [FOCS 2013] to incorporate this code for inapproximability results. In particular, we prove quasi-NP-hardness of the following problems on $n$-vertex hyper-graphs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Coloring a 2-colorable 8-uniform hypergraph with $2^{2^{\Omega(loglog\sqrt{n})}}$ colors.&lt;/li&gt;
&lt;li&gt;Coloring a 4-colorable 4-uniform hypergraph with $2^{2^{\Omega(loglog\sqrt{n})}}$ colors.&lt;/li&gt;
&lt;li&gt;Coloring a 3-colorable 3-uniform hypergraph with $(log n)^{\Omega(1/logloglog n)}$ colors.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In each of these cases, the hardness results obtained are (at least) exponentially stronger than what was previously known for the respective cases. In fact, prior to this result, polylog n colors was the strongest quantitative bound on the number of colors ruled out by inapproximability results for $O(1)$-colorable hypergraphs. The fundamental bottleneck in obtaining coloring inapproximability results using the low- degree long code was a multipartite structural restriction in the PCP construction of Dinur-Guruswami. We are able to get around this restriction by simulating the multipartite structure implicitly by querying just one partition (albeit requiring 8 queries), which yields our result for 2-colorable 8-uniform hypergraphs. The result for 4-colorable 4-uniform hypergraphs is obtained via a &amp;lsquo;query doubling&amp;rsquo; method. For 3-colorable 3-uniform hypergraphs, we exploit the ternary domain to design a test with an additive (as opposed to multiplicative) noise function, and analyze its efficacy in killing high weight Fourier coefficients via the pseudorandom properties of an associated quadratic form.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning</title>
      <link>http://geevi.github.io/project/deep-reinforcement-learning/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/deep-reinforcement-learning/</guid>
      <description>

&lt;h3 id=&#34;students&#34;&gt;Students&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Mohammad Shaffrudin

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://geevi.github.io/pdfs/Mohammed-RL-slides.pdf&#34;&gt;Final slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://geevi.github.io/pdfs/Mohammed-DeepQ-Learning.pdf&#34;&gt;Mid slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Vidit Jain&lt;/li&gt;
&lt;li&gt;Pranav Bhasin&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning Programming &amp; Deployment</title>
      <link>http://geevi.github.io/talk/dl-prog-dep-lead-summ/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0530</pubDate>
      
      <guid>http://geevi.github.io/talk/dl-prog-dep-lead-summ/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Model Compression</title>
      <link>http://geevi.github.io/talk/model-compression-mlschool17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0530</pubDate>
      
      <guid>http://geevi.github.io/talk/model-compression-mlschool17/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hardness of Approximate Coloring</title>
      <link>http://geevi.github.io/publication/thesis/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/thesis/</guid>
      <description>&lt;p&gt;The graph coloring problem is a notoriously hard problem, for which we do not have efficient algorithms. A coloring of a graph is an assignment of colors to its vertices such that the end points of every edge have different colors. A k-coloring is a coloring that uses at most k distinct colors. The graph coloring problem is to find a coloring that uses the minimum number of colors. Given a 3-colorable graph, the best known efficient algorithms output an n0. 199···-coloring. It is known that efficient algorithms cannot find a 4-coloring, assuming &amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Model Compression</title>
      <link>http://geevi.github.io/project/model-compression/</link>
      <pubDate>Mon, 27 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/model-compression/</guid>
      <description>

&lt;h3 id=&#34;students&#34;&gt;Students&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Soham Saha

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://geevi.github.io/pdfs/Soham-Model-Compression.pdf&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://geevi.github.io/pdfs/RDShowcase.pdf&#34;&gt;R &amp;amp; D Showcase &amp;lsquo;16 poster&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Ameya Prabhu

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://geevi.github.io/pdfs/Ameya-indep-study.pdf&#34;&gt;Final Report&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hardness of Approximate Coloring</title>
      <link>http://geevi.github.io/project/hardness/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/hardness/</guid>
      <description>&lt;p&gt;Improved lowerbounds for graph/hypergraph coloring.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Physarum Computer</title>
      <link>http://geevi.github.io/project/physarum-computer/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/physarum-computer/</guid>
      <description>&lt;p&gt;Formal proof for slime-molds finding shortest paths in maze.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probabilitically Checkable Proofs</title>
      <link>http://geevi.github.io/project/pcps/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/pcps/</guid>
      <description>&lt;p&gt;Improved PCPs using low-degree codes and product constructions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Streaming Algorithms</title>
      <link>http://geevi.github.io/project/streaming/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/streaming/</guid>
      <description>&lt;p&gt;Few pass, small memory algorithms for big data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Characterization of Hard-to-cover CSPs</title>
      <link>http://geevi.github.io/publication/characterization-covering/</link>
      <pubDate>Mon, 10 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/characterization-covering/</guid>
      <description>&lt;p&gt;We continue the study of covering complexity of constraint satisfaction problems (CSPs) initiated by Guruswami, Hastad and Sudan [SIAM J. Computing, 31(6):1663&amp;ndash;1686, 2002] and Dinur and Kol [In Proc. 28th IEEE Conference on Computational Complexity, 2013]. The covering number of a CSP instance $\phi$, denoted by ν(Φ) is the smallest number of assignments to the variables of $\phi$, such that each constraint of Φ is satisfied by at least one of the assignments. We show the following results regarding how well efficient algorithms can approximate the covering number of a given CSP instance.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Assuming a covering unique games conjecture, introduced by Dinur and Kol, we show that for every non-odd predicate P over any constant sized alphabet and every integer K, it is NP-hard to distinguish between P-CSP instances (i.e., CSP instances where all the constraints are of type P) which are coverable by a constant number of assignments and those whose covering number is at least K. Previously, Dinur and Kol, using the same covering unique games conjecture, had shown a similar hardness result for every non-odd predicate over the Boolean alphabet that supports a pairwise independent distribution. Our generalization yields a complete characterization of CSPs over constant sized alphabet Σ that are hard to cover since CSP&amp;rsquo;s over odd predicates are trivially coverable with |Σ| assignments.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For a large class of predicates that are contained in the 2k-LIN predicate, we show that it is quasi-NP-hard to distinguish between instances which have covering number at most two and covering number at least Ω(loglogn). This generalizes the 4-LIN result of Dinur and Kol that states it is quasi-NP-hard to distinguish between 4-LIN-CSP instances which have covering number at most two and covering number at least Ω(logloglogn).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>On Fortification of Projection Games</title>
      <link>http://geevi.github.io/publication/fortification/</link>
      <pubDate>Mon, 10 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/fortification/</guid>
      <description>&lt;p&gt;A recent result of Moshkovitz cite{Moshkovitz14} presented an ingenious method to provide a completely elementary proof of the Parallel Repetition Theorem for certain projection games via a construction called fortification. However, the construction used in cite{Moshkovitz14} to fortify arbitrary label cover instances using an arbitrary extractor is insufficient to prove parallel repetition. In this paper, we provide a fix by using a stronger graph that we call fortifiers. Fortifiers are graphs that have both 1 and 2 guarantees on induced distributions from large subsets. We then show that an expander with sufficient spectral gap, or a bi-regular extractor with stronger parameters (the latter is also the construction used in an independent update cite{Moshkovitz15} of cite{Moshkovitz14} with an alternate argument), is a good fortifier. We also show that using a fortifier (in particular 2 guarantees) is necessary for obtaining the robustness required for fortification.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
