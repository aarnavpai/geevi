<!DOCTYPE html>
<html lang="en">


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    
    
    
    
    
    <meta name="generator" content="Hugo 0.80.0" />

    <title>Girish Varma</title>

    
    <link href="/css/bootstrap.min.css" rel="stylesheet">

    
    <link href="/css/scrolling-nav.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Neuton" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <link rel="stylesheet" href="https://girishvarma.in/css/style.css">

</head>



<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Cookie&family=Sacramento&display=swap" rel="stylesheet">

<body id="page-top">
    


    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
        <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="https://girishvarma.in">gv</a>
            <button class="navbar-toggler collapsed" type="button" data-toggle="collapse"
                data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
                aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="navbar-collapse collapse" id="navbarResponsive" style="">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="/#about">Home</a>
                    </li>
                    
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="/#teaching">Teaching</a>
                    </li>
                    
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="/#events">Events</a>
                    </li>
                    
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="/#group">Group</a>
                    </li>
                    
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="/#projects">Research</a>
                    </li>
                    
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="/#publications">Publications</a>
                    </li>
                    
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="/pdfs/girish-resume.pdf">CV</a>
                    </li>
                    
                </ul>
                <a href="/#contact" class="btn btn-outline" role="button">Contact</a>
                
            </div>
        </div>
    </nav>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], }
  });
  </script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <style>
      html, body {
          scroll-behavior: smooth;
      }
  </style>

<div class="container">

  <div class="row">

    
    <div class="col-lg-8 pub" itemscope itemtype="http://schema.org/CreativeWork">


      <div class="pub-title">
        <h1 itemprop="name">Probabilistic Graphical Models</h1>
           </div>
      
      <div class="article-container">

        

        <div class="pub-conf">

          

        </div>
        <div class="visible-xs space-below"></div>

        <div class="pub-date">

          January, 2020

        </div>
        <div class="visible-xs space-below"></div>

        <div class="pub-links" style="padding-top: 10px">


          










        </div>

        <div class="tag-date pub-label">
          
          
          

          <span class="d-inline-block  text-dark tag "> <a href="projects/theory-cs">theory-cs</a></span>
          
          
          
          

          <span class="d-inline-block  text-dark tag "> <a href="projects/machine-learning">machine-learning</a></span>
          
          

        </div>
        <div class="visible-xs space-below"></div>


        <div class="space-below"></div>
        

        <div class="article-style"><p>Probabilistic Graphical Models refers to concise representations of probability distributions using graphs.
It also studies efficient algorithms for sampling distributions represented in such form. Sampling might need to be done from
the joint probability distribution, the marginals or even conditional distributions. Other algorithmic questions involve computing
the Maximum Likelihood Estimate (MLE), Maximum Aposteriori Estimate (MAP) etc. This topic has deep connections and applications to various
fields including Theoretical Computer Science, Machine Learning, Statistical Physics, Bioinformatics etc. We will also be covering analysis of Markov Chain Monte Carlo (MCMC) Algorithms.</p>
<p>Broadly the course will cover four modules</p>
<ol>
<li>Representations</li>
<li>Inference</li>
<li>Learning</li>
<li>Advanced Topics (More on MCMC Methods, Normalizing Flows, Learning theory)</li>
</ol>
<p><a href="pgm_syllabus.pdf">Draft Syllabus</a></p>
<h2 id="grading">Grading</h2>
<table>
<thead>
<tr>
<th>Type of Eval</th>
<th style="text-align:right">&ndash;Weightage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Quiz 1</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td>Mid Sem</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td>Quiz 2</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td>End Sem</td>
<td style="text-align:right">25</td>
</tr>
<tr>
<td>Assignments</td>
<td style="text-align:right">20</td>
</tr>
<tr>
<td>Project</td>
<td style="text-align:right">20</td>
</tr>
</tbody>
</table>
<h2 id="lectures">Lectures</h2>
<h3 id="lec-1-2-probability-recap">Lec 1-2: Probability Recap</h3>
<ul>
<li>
<p><em>Read:</em> For recalling basics of probability and graph theory, please go through [DB] Chapter 1, 2</p>
</li>
<li>
<p><em>Solve:</em>
- <!-- raw HTML omitted -->sub<!-- raw HTML omitted --> For any two distributions $p,q$ on $\{1, \cdots, n \}$, show that:
$$\max_{A \subseteq \{1, \cdots, n\}} | p(A) - q(A) | = \frac{\sum_{i=1}^n | p(i) - q(i) |}{2}.$$
Note that the event $A$ choosen in LHS is a way of distinguishing $p$ from $q$ using $1$ sample in the best possible way. The RHS is the $\ell_1$ norm $|p - q|_1$.
- <!-- raw HTML omitted -->sub<!-- raw HTML omitted --> Prove that any DAG (Directed Acyclic Graph) with finite number of vertices, has atleast one vertex with no incoming edges (ie. pointed towards it). Also show that there is atleast one vertex with no outgoing edges.</p>
<pre><code>    [Hint] Note that there are infinite graphs where the statement is not true. Hence you need to use the fact that the graph has only finite number of nodes in the proof.
</code></pre>
</li>
</ul>
<h3 id="lec-3-belief-networks-i">Lec 3: Belief Networks I</h3>
<p>Free parameters in distributions | Conditional Independence reduces parameters | Graph Representation | d-Connectivity and Independence</p>
<ul>
<li><em>Read:</em> [DB] Sections 3.1 - 3.3</li>
<li><em>Solve:</em>
- [DB] Section 3.8 Exercise 24<!-- raw HTML omitted -->sub<!-- raw HTML omitted -->, 27, 35<!-- raw HTML omitted -->sub<!-- raw HTML omitted -->.</li>
</ul>
<h3 id="lec-4-belief-networks-ii">Lec 4: Belief Networks II</h3>
<p>d-Connectivity  | I-maps | Minimal and Perfect Imaps</p>
<ul>
<li><em>Read:</em>
<ul>
<li>[KE] <a href="https://ermongroup.github.io/cs228-notes/representation/directed/">Bayesian Networks</a></li>
<li>[DB] Section 3.3</li>
</ul>
</li>
<li><em>Explore:</em>
- [KF] Chapter 3. Section 3.3</li>
</ul>
<h3 id="lec-5-markov-networks-i">Lec 5: Markov Networks I</h3>
<ul>
<li><em>Read:</em>
<ul>
<li>[DB] Chapter 4</li>
<li>[KE] <a href="https://ermongroup.github.io/cs228-notes/representation/undirected/">https://ermongroup.github.io/cs228-notes/representation/undirected/</a></li>
</ul>
</li>
</ul>
<h3 id="lec-6-markov-networks-ii">Lec 6: Markov Networks II</h3>
<ul>
<li>
<p><em>Read:</em>
- [DB] Chapter 4
- [KE] <a href="https://ermongroup.github.io/cs228-notes/representation/undirected/">https://ermongroup.github.io/cs228-notes/representation/undirected/</a></p>
</li>
<li>
<p><strong><a href="PGM_Quiz_1.pdf">Quiz I</a></strong></p>
</li>
</ul>
<h3 id="lec-7-inference-i--variable-elimination-and-message-passing">Lec 7: Inference I : Variable Elimination and Message Passing</h3>
<ul>
<li><a href="https://iiitaphyd-my.sharepoint.com/:o:/g/personal/girish_varma_iiit_ac_in/EjDx5EHDB15HsxnQffn_WkoBxcO1nO3cmaNsz7uSe6q8mw?e=IidTZi">Notes</a></li>
<li><em>Read:</em>
<ul>
<li>[DB] Chapter 5</li>
<li>[KE] <a href="https://ermongroup.github.io/cs228-notes/inference/ve/">https://ermongroup.github.io/cs228-notes/inference/ve/</a></li>
</ul>
</li>
</ul>
<h3 id="lec-8-9-markov-chain-monte-carlo-sampling">Lec 8-9: Markov Chain Monte Carlo Sampling</h3>
<ul>
<li><em>Read</em>:
<ul>
<li><a href="https://statweb.stanford.edu/~owen/pubtalks/05mcmc.pdf">Slides by Art Owen</a> (till slide 22)</li>
<li><a href="https://math.uchicago.edu/~shmuel/Network-course-readings/MCMCRev.pdf">MCMC Revolution by Persi Diaconis</a> Pages 1-5 (has cryptography example)</li>
</ul>
</li>
<li><em>Explore</em>:
<ul>
<li><a href="https://statweb.stanford.edu/~owen/mc/Ch-MCMC.pdf">MCMC Book Chapter by Art Owen</a></li>
</ul>
</li>
</ul>
<h3 id="lec-10-tutorial---i">Lec 10: Tutorial - I</h3>
<h3 id="lec-11-probability-basics-of-sampling-tail-bounds">Lec 11: Probability Basics of Sampling: Tail Bounds</h3>
<ul>
<li><em>Read</em>:
<ul>
<li><a href="https://www.cs.princeton.edu/courses/archive/fall16/cos521/Lectures/lec3.pdf">Lectures Notes by Pravesh Kothari</a></li>
</ul>
</li>
</ul>
<h3 id="lec-12-amplification-and-ising-model">Lec 12: Amplification and Ising Model</h3>
<ul>
<li>
<p><em>Read</em>:</p>
<ul>
<li><a href="https://statweb.stanford.edu/~owen/mc/Ch-MCMC.pdf">MCMC book by Art Owen</a>.  Pages 6-8 for Ising Model definition. Pages 16-18 for General MCMC formulation (also known as Metropolis - Hasting&rsquo;s). Pages 31 - 32 for MCMC formulation of Ising Model.</li>
</ul>
</li>
<li>
<p><strong><a href="PGM_MidSem.pdf">Mid Sem Exam</a></strong></p>
</li>
</ul>
<h3 id="lec-13-intro-to-learning-theory">Lec 13: Intro to Learning Theory</h3>
<ul>
<li><em>Read</em>:
<ul>
<li><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding ML by Shai Shalev-Shwartz, Shai Ben-David</a>.  Chapter 2</li>
</ul>
</li>
</ul>
<h3 id="lec-14-agnostic-pac-learning">Lec 14: Agnostic PAC Learning</h3>
<ul>
<li>
<p><em>Read</em>:</p>
<ul>
<li><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding ML by Shai Shalev-Shwartz, Shai Ben-David</a>.  Chapter 3, 4</li>
</ul>
</li>
<li>
<p><strong>Tutorial II on PAC Learning</strong></p>
</li>
</ul>
<h3 id="lec-15-vc-dimension-i">Lec 15: VC Dimension I</h3>
<ul>
<li><em>Read</em>:
- <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding ML by Shai Shalev-Shwartz, Shai Ben-David</a>.  Chapter 6</li>
</ul>
<h3 id="lec-16-vc-dimension-ii---sauer-shellah-peres-lemma">Lec 16: VC Dimension II - Sauer-Shellah-Peres Lemma</h3>
<ul>
<li><em>Read</em>:
<ul>
<li><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding ML by Shai Shalev-Shwartz, Shai Ben-David</a>.  Chapter 6</li>
</ul>
</li>
</ul>
<h3 id="lec-17-recap--learning-conjunctions">Lec 17: Recap &amp; Learning Conjunctions</h3>
<ul>
<li><em>Read</em>:
<ul>
<li><a href="https://pdfs.semanticscholar.org/a3c3/b359f6745b4cfa7238d35b0ce88f4a93c6f1.pdf">Computational Learning Theory by Vazirani, Kearns</a>.  Section 1.3</li>
</ul>
</li>
<li><a href="https://www.youtube.com/watch?v=MEJA_Kf51Xo"><em>Video</em></a></li>
<li><a href="pgm_lec17.pdf"><em>Notes</em></a></li>
</ul>
<h3 id="lec-18-no-free-lunch-theorem">Lec 18: No Free Lunch Theorem</h3>
<ul>
<li><em>Read</em>:
<ul>
<li><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding ML by Shai Shalev-Shwartz, Shai Ben-David</a>.  Chapter 5</li>
</ul>
</li>
<li><a href="pgm_lec18.pdf"><em>Notes</em></a></li>
</ul>
<h3 id="lec-19-learning-graphical-models">Lec 19: Learning Graphical Models</h3>
<ul>
<li><em>Read</em>:
<ul>
<li><a href="https://ermongroup.github.io/cs228-notes/learning/directed/">Learning Bayesian Nets: PGM Course Lecture Notes</a>.</li>
<li><a href="https://ermongroup.github.io/cs228-notes/learning/undirected/">Learning Markov Nets: PGM Course Lecture Notes</a>.</li>
</ul>
</li>
<li><a href="https://www.youtube.com/watch?v=nY15BLUBsbw"><em>Video</em></a></li>
<li><a href="pgm_lec19.pdf"><em>Notes</em></a></li>
</ul>
<h3 id="lec-20-group-testing">Lec 20: Group Testing</h3>
<ul>
<li><a href="https://web.microsoftstream.com/video/fefbbdd0-05e2-45bb-8709-c86a09a312f8"><em>Video</em></a></li>
<li><a href="pgm_lec19.pdf"><em>Notes</em></a></li>
<li><em>Reference</em>
<ul>
<li><a href="https://people.csail.mit.edu/indyk/survey-10.pdf">Sparse Recovery using Sparse Matrices, Section III</a></li>
</ul>
</li>
</ul>
<h2 id="textbook-and-references">Textbook and References</h2>
<ul>
<li>
<p>[DB] <a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf">Bayesian Reasoning and Machine Learning</a>.
David Barber</p>
</li>
<li>
<p>[KE] <a href="https://ermongroup.github.io/cs228-notes/">Probabilistic Graphical Models, Course Notes</a>.
Volodymyr Kuleshov and Stefano Ermon</p>
</li>
<li>
<p>[MMSM] <a href="https://stat.ethz.ch/~maathuis/papers/Handbook.pdf">Handbook of Graphical Models</a>
Marloes Maathuis, Mathias Drton, Steven Lauritzen, Martin Wainwright</p>
</li>
<li>
<p>[KF] Probabilistic Graphical Models: Principles and Techniques.
Daphne Koller and Nir Friedman, MIT Press (2009).</p>
</li>
<li>
<p>[EX] <a href="https://www.cs.cmu.edu/~epxing/Class/10708-17/lecture.html">Probabilistic Graphical Models</a>
Eric Xing</p>
</li>
<li>
<p>[KM] Machine Learning: a Probabilistic Perspective
by Kevin Patrick Murphy</p>
</li>
<li>
<p>[WJ] <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a>  Martin J. Wainwright and Michael I. Jordan</p>
</li>
<li>
<p>[MM] <a href="https://web.stanford.edu/~montanar/RESEARCH/book.html">Information, Physics, and Computation</a>
Marc Mézard and Andrea Montanari</p>
</li>
</ul>
</div>


      </div>
    </div>
    
    <div class="col-lg-4 pub" itemscope itemtype="http://schema.org/CreativeWork">

        
        
        <aside style="border: 1px #cccccc solid;
        margin-left: 10px; padding-left: 10px;     background: #f1f1f1;">
        <b>Contents</b>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#grading">Grading</a></li>
    <li><a href="#lectures">Lectures</a>
      <ul>
        <li><a href="#lec-1-2-probability-recap">Lec 1-2: Probability Recap</a></li>
        <li><a href="#lec-3-belief-networks-i">Lec 3: Belief Networks I</a></li>
        <li><a href="#lec-4-belief-networks-ii">Lec 4: Belief Networks II</a></li>
        <li><a href="#lec-5-markov-networks-i">Lec 5: Markov Networks I</a></li>
        <li><a href="#lec-6-markov-networks-ii">Lec 6: Markov Networks II</a></li>
        <li><a href="#lec-7-inference-i--variable-elimination-and-message-passing">Lec 7: Inference I : Variable Elimination and Message Passing</a></li>
        <li><a href="#lec-8-9-markov-chain-monte-carlo-sampling">Lec 8-9: Markov Chain Monte Carlo Sampling</a></li>
        <li><a href="#lec-10-tutorial---i">Lec 10: Tutorial - I</a></li>
        <li><a href="#lec-11-probability-basics-of-sampling-tail-bounds">Lec 11: Probability Basics of Sampling: Tail Bounds</a></li>
        <li><a href="#lec-12-amplification-and-ising-model">Lec 12: Amplification and Ising Model</a></li>
        <li><a href="#lec-13-intro-to-learning-theory">Lec 13: Intro to Learning Theory</a></li>
        <li><a href="#lec-14-agnostic-pac-learning">Lec 14: Agnostic PAC Learning</a></li>
        <li><a href="#lec-15-vc-dimension-i">Lec 15: VC Dimension I</a></li>
        <li><a href="#lec-16-vc-dimension-ii---sauer-shellah-peres-lemma">Lec 16: VC Dimension II - Sauer-Shellah-Peres Lemma</a></li>
        <li><a href="#lec-17-recap--learning-conjunctions">Lec 17: Recap &amp; Learning Conjunctions</a></li>
        <li><a href="#lec-18-no-free-lunch-theorem">Lec 18: No Free Lunch Theorem</a></li>
        <li><a href="#lec-19-learning-graphical-models">Lec 19: Learning Graphical Models</a></li>
        <li><a href="#lec-20-group-testing">Lec 20: Group Testing</a></li>
      </ul>
    </li>
    <li><a href="#textbook-and-references">Textbook and References</a></li>
  </ul>
</nav>
        </aside>
        
            </div>

  </div>
</div>


    
    <footer class="py-5 bg-dark">
        <div class="container">
          <p class="m-0 text-center text-white">
              © 2017 Girish Varma · Powered by the Research Portfolio theme for Hugo.</p>
        </div>
        
      </footer>

      
      <script src="/js/jquery.min.js"></script>
      <script src="/js/bootstrap.bundle.min.js"></script>

      
      <script src="/js/jquery.easing.min.js"></script>

      
      <script src="/js/scrolling-nav.js"></script>

      <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,400b" rel="stylesheet">


</body></html>